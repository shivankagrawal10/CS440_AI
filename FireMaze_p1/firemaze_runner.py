import numpy as np
import random
import heapq
import math
import firemaze as mg
import constants
import copy
import matplotlib.pyplot as plt

#Experiment Class has framework to test the firemaze by creating an instance of a firemaze object, updating the fire
#in each time step, and taking in user input for the type of strategy to use

class experiment:
    #Experiment constructor
    #@param takes in probability density of barrier (p), flammability quotient (q), start coordinate, end coordinate, strategy, and optional seed
    # Strategy: {1:(Astar search 1 time), 2:(Astar search at each step), 3:(Marco Polo search with large steps), 4:(Marco Polo Probability with large steps) 
    def __init__(self, dim : int, p : float, q : float, start : (int, int), end : (int, int), strategy: int, seed=-1):
        self.seed = seed
        if(seed>=0):
            self.rg=random.Random(seed)
            self.maze = mg.maze(dim, p, q,seed=self.rg.randint(0,100))
        else:
            self.rg=random.Random()
            self.maze = mg.maze(dim, p, q)
        #Creating seed object to ensure we can recreate mazes and fire scenarios when testing
        self.q = q
        self.start = start
        self.end = end
        y, x = start
        self.agent = (y, x)
        self.maze.grid[y][x] = constants.AGENT
        self.strategy = strategy
        self.switch = False
        self.neighbor_prob={}

    #Starts the firemaze environment and executes the planning and travel of strategy
    def run(self,animate):
        if not self.maze.fireloc:
            self.start_fire()
        plan = []
        while self.agent != self.end:
            if(animate):
                if(animate==1):
                    self.maze.maze_visualize(self.agent,self.maze.grid,0)
                else:
                    self.maze.maze_visualize(self.agent,self.maze.grid,2)
            #input("Press any key to continue") #optional if want to see maze for longer
            plan = self.generate_plan(self.strategy, plan)
            if not plan:
                break
            plan = self.execute_plan(self.strategy, plan)
        if(animate):
            if(animate==1):
                self.maze.maze_visualize(self.agent,self.maze.grid,1)
            else:
                self.maze.maze_visualize(self.agent,self.maze.grid,3)
        if self.agent == self.end:
            return True
        else:
            return False

    #Generates a path the agent should follow based on selected strategy 
    #if not a valid strategy or agent on fire then will return an empty plan
    #@return plan (list object)
    def generate_plan(self, strategy, plan):
        if self.maze.grid[self.agent[0]][self.agent[1]] == constants.FIRE:
            return []
        if strategy == constants.STRATEGY_1 and not plan:
            #A Star based on travel cost and euclidean distance heuristic
            plan, _ = self.maze.Astar(self.agent, self.end)
        elif strategy == constants.STRATEGY_2:
            #A Star based on travel cost and euclidean distance heuristic
            plan, _ = self.maze.Astar(self.agent, self.end)
        elif strategy == constants.STRATEGY_3:
            #A Star based on traditional distance heuristic and distance from fire
            plan, _ = self.maze.Marco_Polo(self.agent, self.end)
        elif strategy == constants.STRATEGY_4 or strategy == constants.STRATEGY_5:
            #A Star based on traditional distance heuristic, distance from fire, and probability of neighbor catching on fire
            plan, _ =  self.maze.Marco_Polo_Prob(self.agent, self.end)
        elif strategy == constants.STRATEGY_6:
            # Simulate paths to end, and weight traditional distance, distance from fire heuristic with simulated probability
            # Runs the whole path after doing simulations 
            best_prob , best_first_step = self.simulation()
            plan, _ = self.maze.Marco_Polo_Prob(self.agent, self.end,self.neighbor_prob)
        return plan

    #Responsible for running the plan generated by the search strategy
    #modularizing the running from planning enables us to experiment with number of steps taken before searching again
    def execute_plan(self, strategy, plan):
        times = 0
        if strategy == constants.STRATEGY_1 or strategy == constants.STRATEGY_6:
            times = len(plan) - 1
        elif strategy == constants.STRATEGY_2 :
            times = 1
        elif strategy == constants.STRATEGY_3 or strategy == constants.STRATEGY_4:
            times = max(int((self.maze.dist_to_nearest_fire(plan[0]))/2),1)
        elif strategy == constants.STRATEGY_5:
            times = int((self.maze.dist_to_nearest_fire(plan[0]))/2)
            if times <= 1:
                best_prob , best_first_step = self.simulation()
                plan,_ = self.maze.Marco_Polo_Prob(self.agent, self.end,self.neighbor_prob)
                self.neighbor_prob={}
                times= len(plan) - 1
        
        for i in range(times):
            if self.agent == self.end: 
                plan = []
                break
            if plan:
                curr = plan.pop(0)
                y, x = plan[0]
                if self.maze.grid[y][x] == constants.FIRE:
                    plan = []
                    break
                else:
                    old_y, old_x = self.agent
                    new_y, new_x = self.agent = plan[0]
                    self.maze.grid[old_y][old_x] = constants.OPEN
                    self.maze.grid[new_y][new_x] = constants.AGENT
            else:
                break
            new_grid = self.advance_fire()
            y, x = self.agent
            self.maze.grid = new_grid
            if new_grid[y][x] == constants.FIRE:
                    plan = []
                    break
        return plan

    #Spreads the fire in the graph environment according to probability formula given in problem statement
    def advance_fire(self):
        clone = self.maze.clone_grid()
        for i in range(self.maze.dim):
            for j in range(self.maze.dim):
                if clone[i][j] != constants.FIRE and clone[i][j] != constants.BLOCKED:
                    prob = self.maze.get_fire_prob((i, j))
                    if prob != 0 and self.rg.random() <= prob:
                        clone[i][j] = constants.FIRE
                        self.maze.fireloc.append((i,j))
        return clone

    #Randomly selects a start position for the fire from the open nodes
    def start_fire(self):
        open_cells = []
        for i in range(self.maze.dim):
            for j in range(self.maze.dim):
                if self.maze.grid[i][j] == constants.OPEN:
                    open_cells.append((i, j))
        y, x = self.rg.choice(open_cells)
        self.maze.grid[y][x] = constants.FIRE
        self.maze.fireloc.append((y,x))
        return ((y,x))

    #Simulates reaching the goal from the current node's neighbors
    def simulation(self):
        best = (0, self.agent)
        neighbors = self.maze.get_neighbors(self.agent,self.maze.is_open)
        neighbors = list(neighbors)
        for n in neighbors:
            p = self.get_probability(n)
            self.neighbor_prob[n]=p
            if p > best[0]:
                best = (p, n)
        return best

    #Creates copy of current maze and calculates the probability of success when starting from a particular node using simulation
    def get_probability(self, start: (int, int)):
        success = 0
        times=10
        for i in range(times):
            sim = experiment(self.maze.dim,self.maze.p,self.q,
                            start,self.end,constants.STRATEGY_3)
            sim.maze.grid = self.maze.clone_grid()
            sim.maze.fireloc = copy.deepcopy(self.maze.fireloc)
            sim.agent = start
            sim.advance_fire()
            if sim.man_run(0):
                success += 1
        return float(success / times)


#Debugging tests
'''
dim=15

for i in range(3):   
    random.seed(10*i)
    exp = experiment(dim, .2 , .2, (0, 0), (dim-1, dim-1), 4,seed=random.randint(0,10))
    #if i==2:
    exp.run(1)
'''